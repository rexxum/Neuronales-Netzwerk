{
 "cells": [
  {
   "attachments": {
    "image-2.png": {
     "image/png": "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"
    },
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "b21aec63",
   "metadata": {},
   "source": [
    "# Neuronales Netzwerk\n",
    "\n",
    "\n",
    "## Einleitung\n",
    "\n",
    "Seit jeher versucht man die rätselhaften und unerklärlichen Prozesse zu verstehen, welche die Natur mitsich bringt. Sei es auf physikalischer, chemischer oder biologischer Ebene. Zu diesen faszinierenden Themen gehören für mich auch die spannenden Prozesse des menschlichen Gehirns. Es ist das komplizierteste Organ, welches grundsätzlich für die Aufnahme und Verarbeitung von Signalen zuständig ist und besteht aus mehreren Milliarden von Neuronen. Um zu verstehen wie ein solch grosses biologisches Neuronales Netzwerk funtioniert, haben Wissenschaftler bereits vor vielen Jahren den Versuch gestartet, diese hochkomplexen Vorgänge vereinfacht als mathematische Formeln darzustellen und zu beschreiben. Diese Modelle nannte man anschliessend Künstliche Neuronale Netzwerke, da sie auf den Prozessen eines echten Gehirns basieren. Heute stehen wir an einem ganz anderen Punkt bezüglich der Entwicklung von diesen Künstlichen Neuronalen Netzwerken, dank der Neurowissenschaften. Trotzdem gibt es immernoch viele Unklarheiten und Mysterien welche die Forschungswelt beschäftigen. Das Ziel dieser Arbeit ist es einige dieser komplexen Vorgänge von KNNs herunterzubrechen und zu erklären auch anhand einer vereinfachten Anleitung eines Künstlichen Neuronalen Netzwerks, welches Ziffern von null bis neun klassifizieren kann.\n",
    "\n",
    "## Perzeptron\n",
    "\n",
    "Um diese Vorgänge zu verstehen möchte ich zuerst ganz kurz auf das einfachste KNN zusprechen kommen. Das Perzeptron. Es stellt ein Modell eines einzigen Neurons dar, welches Eingaben erhält, diese verarbeitet und eine Ausgabe produziert.\n",
    "\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "\n",
    "Im folgenden Abschnitt wird kurz erklärt wie ein solches Perzeptron funktioniert. Sobald man den Vorgang eines einzigen Neurons verstanden hat, wird es logischerweise einfacher Neuronale Netzwerke zu verstehen, da diese im Grunde genommen einfach aus vielen verketteten Neuronen bestehen. \n",
    "\n",
    "![image-2.png](attachment:image-2.png)\n",
    "\n",
    "Wie bereits erwähnt besteht ein Perzeptron aus einem einzigen Neuron. Dieses Neuron erhält Eingaben z.B. in Form einer Matrix und produziert anschliessend mithilfe eines bestimmten Algorithmus eine Ausgabe. Im Falle einer falschen Ausgabe so müssen einzelne Parameter des Algorithmus angepasst werden, um das richtige Ergebnis zu erhalten. Dieser Prozess ist einigermassen anspruchsvoll und wird \"Gradient descent\" genannt. Vorerst muss jedoch aufgeklärt werden, was mit dem \"Algorithmus\" gemeint ist. Dies ist nicht allzu kompliziert und ist in 2 Abschnitte unterteilbar.\n",
    "<a id=\"1\"></a>\n",
    "#### Schritt 1: Summe\n",
    "\n",
    "Wie die untenstehende Abbildung zeigt, hat das Perzeptron gewisse Eingaben auch Inputs genannt. In diesem Falle gibt es die Inputs [I0] und [I1]. Die Inputs werden danach mit individuellen Gewichten multiplizert und anschliessend wird die Summe aller Multiplikationen berechnet. In unserem Beispiel sähe dies folgendermassen aus:\n",
    "$$[I_0] \\times [w_0] + [I_1] \\times [w_1] = x$$\n",
    "\n",
    "Dies ist der erste Schritt, welcher der Algorithmus durchführt. Die gewichtete Summe aller Inputs sozusagen. Diese Operation wirkt sehr einfach und grundsätzlich ist sie auch einfach, jedoch sollte man im Hinterkopf behalten, dass dieses Beispiel lediglich 2 Inputs beinhaltet. Der Vorgang bleibt aber der gleiche egal ob 2 oder 10'000 Inputs.\n",
    "<a id=\"2\"></a>\n",
    "#### Schritt 2: Aktivierungsfunktion\n",
    "\n",
    "Der zweite Schritt beinhaltet eine sogenannte **Aktivierungsfunktion**. Wichtig ist anzumerken, dass die Aktivierungsfunktion ein Kerngedanke darstellt, wenn es um Künstliche Neuronale Netzwerke geht und alle KNNs besitzen solche Aktivierungsfunktionen. Grundsätzlich erlaubt einem die Aktivierungsfunktion, die Ausgabe auf einen bestimmten Bereich anzupassen. z.B. gibt es eine Aktivierungs Funktion welche nur Zahlen im Bereich zwischen 0 und 1 Zahlen erzeugt, um zu grosse Zahlen zu vermeiden.\n",
    "\n",
    "In unserem Beispiel kann man es sich einfach so vorstellen, dass die Summe aller Multiplikationen und Gewichten aus Schritt 1 ($x$), nun an die Aktivierungsfunktion weitergereicht wird. Diese Funktion erzeugt dann die finale Ausgabe. Logischerweise gibt es verschiedenste Aktivierungsfunktionen. Es kommt ganz auf den spezifischen Verwendungszweck an. Bei einem linearen Klassifizierungs Problem macht es beispiesweise keinen Sinn, eine komplizierte non-lineare Funktion zu wählen. \n",
    ">Bild welches zu den Beschreibungen passt\n",
    "\n",
    ">evtl animation von Perzeptron\n",
    "\n",
    "Das Grundprinzip eines Neurons ist jetzt sicher einmal verständlich geworden.\n",
    "***\n",
    "## Mathematik\n",
    "Um Neuronale Netzwerke verstehen zu können, sind verschiedenste mathematische Grundlagen von grosser Bedeutung. Deshalb werden die wichtigsten im folgenden Abschnitt kurz aufgezeigt und erklärt. \n",
    "  \n",
    "  <details>\n",
    "  <summary><font color=\"red\"><b>Hier findet man die ganze Mathematik</b></font> </summary>\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "  #### <u>Vektoren</u>\n",
    "\n",
    "Beginnen wir mit Vektoren. Es muss gesagt werden, dass wir schlussendlich Matrizen benötigen werden, der Einfachheit halber werde ich aber zuerst Vektoren erklären. D.h. wer sich bereits mit Vektoren auskennt, kann diesen Abschnitt ohne Probleme überspringen. Das Verständnis von Vektoren ist ausschlaggebend, wenn man ein Neuronale Netzwerk programmieren will. Aus diesem Grunde sind Vektoren sehr wichtig. Vektoren kann man als Pfeile darstellen, haben eine Länge und eine Orientierung in eine bestimmte Richtung. In diesem Kontext verwenden wir einen Vektor algebraisch \n",
    "(mathematisch) als eine Reihe von Zahlen in Klammern und knapp gesagt sind Vektoren in Python einfache Listen. Dies steht im Gegensatz zur Sichtweise der Physik, wo die Darstellung des Vektors normalerweise als Pfeil gesehen wird, der durch eine Länge und eine Richtung charakterisiert ist.  Allgemein muss man sich Vektoren als eine n-dimensionale Liste mit Werten vorstellen. Beispielsweise: \n",
    "\n",
    "$$\\begin{bmatrix} a_1 & a_2 & ...& a_n \\end{bmatrix} $$    \n",
    "\n",
    "Mit Vektoren sind ganz viele mathematische Operationen möglich.\n",
    "Wir benötigen jedoch nicht alle sondern im Moment nur das Skalarprodukt oder auch Punktprodukt.\n",
    "\n",
    "\n",
    "  #### <u>Punktprodukt</u>\n",
    "\n",
    "Das Dotproduct ist nicht schwer zu verstehen und es ist vom Prinzip derselbe Vorgang, wie die oben besprochene gewichtete Summe aller Inputs in ein Perzeptron. Man nimmt zum Beispiel zwei 2D Vektoren: [2,3] und [3,4]. Nun multipliziert man die beiden Zahlen, mit derselben Position der jeweiligen Vektoren. D.h. in diesem Beispiel nimmt man die erste Stelle des ersten Vektors; die Zahl 2 und multipliziert diese mit der ersten Stelle des zweiten Vektors; die Zahl 3. undso weiter, bis man das Produkt aller Positionen berechnet hat. In unserem Fall bedeute das lediglich noch \"3 * 4\" zu berechen. Sobald man das Produkt aller Positionen hat, berechnet man bloss noch die Summe aller Produkte. D.h. (2 * 3)+(3 * 4)= 18.\n",
    "Das Ziel des Skalarprodukts ist es, aus zwei Vektoren einen Skalar zu bekommen.\n",
    "\n",
    "die entsprechende Formel:\n",
    "    \n",
    "$$ \\vec a \\cdot \\vec b = \\sum_{i=1}^n a_i b_i = a_1b_1 + a_2b_2 + ... + a_nb_n$$\n",
    "    \n",
    "Dieser Vorgang ist von grosser Bedeutung und der Grund warum er hier erklärt wird, ist, weil man diesen Vorgang später wiederbraucht einfach mit Matrizen anstelle von Vektoren.\n",
    "\n",
    "Doch die Frage stellt sich nun, weshalb ist das Verständnis von Vektoren so wichtig im Zusammenhang mit Neuronalen Netzwerken?\n",
    "\n",
    ">animation von vektor in 2D\n",
    "\n",
    "Die Animation zeigt, dass es möglich ist mit der Anpassung von einzelnen Parameter eines Vektors, den Zielvektor zu erhalten. Diese Vorstellung wird analog im Bereich der Neuronalen Netzwerke verwendet. Mithilfe der stetigen Anpassung gewisser Parameter, das erwünschte Ergebnis zu erzielen.\n",
    "Als nächstes möchte ich aber auf Matrizen zusprechen kommen, denn wenn man ein neuronales Netz programmiert, hat man es nicht mehr mit Vektoren sondern mit Matrizen zu tun.\n",
    "\n",
    "\n",
    "  #### <u>Martrizen</u>\n",
    "\n",
    "Wir wollen versuchen zu verstehen, wie diese Operationen funktionieren, sobald wir anfangen Zahlen in Matrizen zu schreiben. Warum ist das nötig? Hierfür gibt es unzählige Gründe. Ein Beispiel sind die Bildschirmpixel, welche als Matrix geschrieben sind. Weiter sind beispielsweise Daten einer Tabelle in Excel in einer Matrix gespeichert. Oder die Gewichte zwischen Neuronen in einem Neuronalen Netzwerk können in Matrizen gespeichert werden. Es gibt ganz viele Fälle, in welchen die Zahlen, die wir bearbeiten wollen als Matrix gespeichert sind. Was ist eine Matrix überhaupt? Eine Matrix ist im Gegensatz zum Vektor keine lineare Liste, sondern ein 2 dimensionales Raster aus Werten. Beispielsweise: \n",
    ">Bild von Matrix\n",
    "\n",
    "Diese Matrix hat die Form (X, Y), da sie X Reihen und Y Spalten hat. \n",
    "\n",
    "Nun soll erklärt werden wie man Matrizen multipliziert, auch Matrizenmultiplikation genannt.\n",
    "\n",
    "\n",
    "  #### <u>Martrizenmultiplikation</u>\n",
    "\n",
    "Das Matrixprodukt ist eine Operation, bei der wir 2 Matrizen haben, und wir Skalarprodukte\n",
    "aller Kombinationen von Zeilen der ersten Matrix und den Spalten der zweiten Matrix durch. \n",
    "Das Ergebnis ist eine Matrix mit diesen atomaren Skalarprodukten. Die Matrixmultipliaktion ist nur möglich, wenn die Anzahl Zeilen der ersten Matrix mit der Anzahl Spalten der Zweiten übereinstimmen. Ein bedeutsamer Unterschied zur Vektorenmultiplikation mit dem Skalarprodukt ist, dass bei der Matrizenmultiplikation als Produkt eine neue Matrix entsteht und nicht eine Zahl. Das Skalarprodukt spielt aber wie erwähnt eine grosse Rolle bei der Matrixmultiplikation. Sie funktioniert wie folgt: Man verwendet das Skalarprodukt in dem man die Einträge der ersten Zeile der ersten Matrix mit den entsprechenden Einträgen der ersten Spalte der zweiten Matrix multipliziert. Anschliessend summiert man alle Produkte und erhält einen Skalar. Diese Zahl wird dann an erster Stelle in der Ergebnismatrix eingetragen. Danach geht man einfach eine Spalte weiter in der zweiten Matrix und wiederholt den Prozess usw. bis zur letzen Spalte in der zweiten Matrix. Anschliessend geht man in der ersten Matrix eine Zeile nach unten und beginnt wieder von vorne, d.h. man nimmt das Skalarprodukt der zweiten Zeile der ersten Matrix und der ersten Spalte der zweiten Matrix. Dieses Vorgehen wird wiederholt bis zum Schluss. \n",
    "\n",
    "Man hat zum Beispiel die Matrix X und Y\n",
    "\n",
    "$$X : \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix} Y : \\begin{bmatrix} 7 & 5 \\\\ 3 & 2 \\\\ 4 & 6 \\end{bmatrix}$$\n",
    "\n",
    "Wie erwähnt sollten die Anzahl Spalten der ersten Matrix und die Anzahl Zeilen der zweiten Matrix übereinstimmen. Die erste Matrix (X) besitzt zwei Zeilen und die zweite (Y) besitzt zwei Spalten. Nun wendet man das Skalarprodukt auf die erste Zeile bzw. Spalte an. Das sieht in unserem Beispiel folgendermassen aus.\n",
    "\n",
    "$$\\begin{bmatrix}\\color {Blue}1&\\color {Blue} 2 &\\color {Blue} 3 \\\\ 4 & 5 & 6 \\end{bmatrix} \\cdot \\begin{bmatrix}\\color {Red}7&5 \\\\\\color {Red}3&2 \\\\\\color {Red}4&6 \\end{bmatrix} = \\begin{bmatrix}\\color {Blue} 1\\times \\color {Red}7 +\\color {Blue} 2\\times \\color {Red}3 + \\color {Blue}3\\times \\color {Red}4 & - \\\\ - &- \\end{bmatrix} = \\begin{bmatrix} \\textbf{25} & - \\\\ - & -  \\end{bmatrix}$$\n",
    "\n",
    "Grundsätzlich geht man bei der Matrixmultiplikation einfach eine Spalte weiter in der zweiten Matrix und wendet das Skalarprodukt an. In unserem Beispiel sieht das so aus:\n",
    "\n",
    "$$\\begin{bmatrix}\\color {Blue}1&\\color {Blue} 2 &\\color {Blue} 3 \\\\ 4 & 5 & 6 \\end{bmatrix}  \\cdot \\begin{bmatrix}7&\\color {Red}5 \\\\3&\\color {Red}2 \\\\4&\\color {Red}6 \\end{bmatrix} = \\begin{bmatrix}- & \\color {Blue} 1\\times \\color {Red}5 +\\color {Blue} 2\\times \\color {Red}2 + \\color {Blue}3\\times \\color {Red}6 \\\\ - &- \\end{bmatrix} = \\begin{bmatrix} 25 & \\textbf{27} \\\\ - & -  \\end{bmatrix}$$\n",
    "\n",
    "Nun gibt es keine weiteren Spalten mehr in Y, das heisst wir gehen zur nächsten Zeile in X über und wiederholen den Vorgang.\n",
    "\n",
    "$$\\begin{bmatrix}1& 2 & 3 \\\\ \\color {Blue}4 & \\color {Blue}5 & \\color {Blue}6 \\end{bmatrix}  \\cdot \\begin{bmatrix}\\color {Red}7&5 \\\\\\color {Red}3&2 \\\\\\color {Red}4&6 \\end{bmatrix} = \\begin{bmatrix}25 & 27\\\\ \\color {Blue}4\\times \\color {Red}7+ \\color {Blue}5\\times \\color {Red}3 +\\color {Blue} 6\\times \\color {Red}4 & - \\end{bmatrix} = \\begin{bmatrix} 25 & 27\\\\ \\textbf{67} & -  \\end{bmatrix}$$\n",
    "\n",
    "Insgesamt erhält man folgende Lösung:\n",
    "\n",
    "$$\\begin{bmatrix} 1\\times 7 + 2\\times 3 + 3\\times 4 & 1\\times 5 + 2\\times 2 + 3\\times 6\\\\ 4\\times 7+ 5\\times 3 + 6\\times 4 & 4\\times 5 + 5\\times 2 + 6\\times 6 \\end{bmatrix} = \\begin{bmatrix} 25 & 27 \\\\ 67 & 66  \\end{bmatrix}$$\n",
    "    \n",
    "***\n",
    "\n",
    "An dieser Stelle möchte ich auf eine hilfreiche [Website](http://matrixmultiplication.xyz/) verweisen, welche das Matrixprodukt gut veranschaulicht.\n",
    "***\n",
    "\n",
    "Was ist jetzt aber, wenn wir zum Beispiel Eingaben in der Form einer (3,2) Matrix haben und unsere Gewichte, an welchen wir das Matrixprodukt anwenden wollen, haben dieselbe Form (3,2). Dafür gibt es eine einfache Lösung. Wie transponieren eine Matrix, sodass sich ihre Form von (3,2) zu (2,3) ändert.\n",
    "<a id=\"3\"></a>\n",
    "#### Transponierung/ Transposition\n",
    "\n",
    "Nun kommt noch eine Operation dazu - Transponierung. Die Transposition verändert einfach eine \n",
    "Matrix so, dass ihre Zeilen zu Spalten und Spalten zu Zeilen werden: \n",
    "\n",
    "$$ \\begin{bmatrix} 1 & 2 & 9 \\\\ 4 & 5 & 6 \\\\3 & 7 & 2 \\end{bmatrix}^T =\\begin{bmatrix} 1 & 4 & 3 \\\\ 2 & 5 & 7 \\\\9 & 6 & 2 \\end{bmatrix}$$\n",
    "\n",
    "Zuerst möchte ich die Operation aber anhand von Vektoren erklären. Es gibt Zeilenvektoren und Spaltenvektoren. Ein Zeilenvektor ist im Grunde auch eine Matrix, deren erste Dimension (die Anzahl der Zeilen) einfach gleich 1 ist und deren \n",
    "Größe der zweiten Dimension (die Anzahl der Spalten) gleich n ist - die Größe des Vektors. Die Form eines Zeilenvektors ist also (1, n):\n",
    "$$\\vec{a} = \\begin{bmatrix} a_1 & a_2 & ...& a_n\\end{bmatrix}$$\n",
    "\n",
    "Die andere Möglichkeit eines Vektors ist der Spaltenvektor. Ein Spaltenvektor ist eine Matrix, bei der die Größe der zweiten Dimension (die Anzahl der Spalten) gleich 1 ist, d.h. er hat die Form (n, 1):\n",
    "$$\\vec{b} = \\begin{bmatrix} b_1 \\\\ b_2 \\\\ ...\\\\ b_n\\end{bmatrix}$$\n",
    "\n",
    "Der Vektor b kann genau gleich erstellt werden wie der Vektor a, nur muss er zudem transponiert werden. Die Transposition verwandelt einfach Zeilen in Spalten und Spalten in Zeilen:\n",
    "\n",
    "$$ \\begin{bmatrix} b_1 & b_2 & ...& b_n\\end{bmatrix}^T = \\begin{bmatrix} b_1 \\\\ b_2 \\\\ ...\\\\ b_n\\end{bmatrix}$$\n",
    "\n",
    "Umgekehrt geht das natürlich genauso:\n",
    "$$ \\begin{bmatrix} b_1 \\\\ b_2 \\\\ ...\\\\ b_n\\end{bmatrix}^T = \\begin{bmatrix} b_1 & b_2 & ...& b_n\\end{bmatrix} $$\n",
    "\n",
    "Das Transponieren  einer Matrix funktioniert analog, es werden einfach Zeilen und Spalten getauscht. Speziell ist, bei einer quadratischen Matrix, so wie die obige, werden alle Einträge einfach an der Hauptdiagonale gespiegelt. Um auch noch eine andere Möglichkeit demonstrieren. Eine Matrix von der Form (2, 3) wird transponiert, dabei entsteht eine Matrix der Form (3, 2):\n",
    "\n",
    "$$ \\begin{bmatrix} 1 & 2 & 9 \\\\ 4 & 5 & 6 \\end{bmatrix}^T = \\begin{bmatrix} 1 & 4  \\\\ 2 & 5 \\\\9 & 6  \\end{bmatrix}$$\n",
    "    \n",
    "    \n",
    "#### <u> Calculus</u>\n",
    "    \n",
    "<a id=\"5\"></a>    \n",
    "    **1. Power Rule:**\n",
    "    \n",
    "$f(x) = x^n$ dann ist die Ableitung: $f'(x) = n\\times x^{n-1}$\n",
    "    \n",
    "\n",
    "<a id=\"4\"></a>    \n",
    "    **2. Chain Rule**\n",
    " \n",
    "Angenommen man hat die Funktionen $y = x^2$ und $x = z^2$. Man möchte die Ableitung von y abhängig von z:\n",
    "    \n",
    "$$\\frac{\\partial y}{\\partial z}$$\n",
    "    \n",
    "Nun kann man zuerst die Ableitung von y abhängig von x berechnen:\n",
    "    \n",
    "    \n",
    "$$\\frac{\\partial y}{\\partial x} = 2x$$\n",
    "    \n",
    "Dann die Ableitung von x abhängig von z:\n",
    "    \n",
    "$$\\frac{\\partial x}{\\partial z} = 2z$$\n",
    "    \n",
    "und diese beiden dann miteinander multiplizieren:\n",
    "    \n",
    "$$ \\frac{\\partial y}{\\partial x} \\times  \\frac{\\partial x}{\\partial z}      =2x \\times 2z $$\n",
    "    \n",
    "Das bedeutet man kann Ableitungen \"aneinanderketten\".<br>\n",
    "Für ein tieferes Verständnis von Differenzialrechnungen empfehle ich die Videos von 3Blue1Brown. Sie sind sicherlich sehr hilfreich, um sich ein solides Grundwissen anzueignen.\n",
    " - [Essence of Calculus](https://www.youtube.com/playlist?list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr)\n",
    "\n",
    "</details>\n",
    "\n",
    "***\n",
    "\n",
    "Das Ziel dieser Anleitung ist es ein einfaches Neuronales Netzerk zu programmieren. Die Schwierigkeit besteht jedoch darin, dass das neuronale Netzwerk ganz ohne Deep Learning-Frameworks wie TensorFlow, Keras oder Pytorch enstehen soll. \n",
    "\n",
    "Ich möchte mit einem kurzen Überblick beginnen, um zu veranschaulichen, was auf uns zu kommt und zugleich um ein wenig Struktur in die Arbeit zu bringen.\n",
    "\n",
    "## Übersicht\n",
    "\n",
    "1. Grundgerüst erstellen: Die nötigen Funktionen festlegen\n",
    "2. Parameter Initialisieren: Die Anzahl der Neuronen und sonstige Parameter werden festgelegt\n",
    "3. Forward Propagation: Die Daten werden durch das Neuronale Netzwerk fliessen und eine Ausgabe ergeben\n",
    "4. Backward Propagation: Die wahrscheinlich falsche Ausgabe mithilfe der Gewichte verbessern.\n",
    "5. Daten importieren\n",
    "6. Training des neuronalen Netzwerkes\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d36e6b",
   "metadata": {},
   "source": [
    "## Grundgerüst\n",
    "Beginnen wir mit einem Grundgerüst für unseren Code. Das heisst wir überlegen kurz, welche Funktionen wir benötigen werden. Im ersten Schritt benötigen wir sicherlich eine Funktion, welche unsere Parameter initialisert. Diese nenne ich: <code>init_params</code>. Als nächstes benötigen wir eine Funktion, welche die forwardpropagation durchführt, das heisst eine erste Ausgabe liefert. Ich gebe ihr den Namen: <code>forward_prop</code>. Die Ausgabe dieser Funktion ist sicherlich noch nicht korrekt, deshalb benötigen wir zum Schluss noch eine Funktion, welche unsere Ausgabe verbessert. Das ist die Ausgabe der backwardpropagation. Deshalb nenne ich sie: <code>backward_prop</code>. Kombiniert ergibt das folgendes Ergebnis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0df4d8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params():\n",
    "    return\n",
    "\n",
    "def forward_prop():\n",
    "    return \n",
    "\n",
    "def backward_prop():\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d2eee5",
   "metadata": {},
   "source": [
    "## Initialisierung der Parameter\n",
    "Starten wir mit der Initialisierung unserer Parameter. Das heisst wir erstellen eine Funktion, welche unsere Verknüpfungsgewichte zwischen den Schichten festlegen soll. Die Dimensionen der Gewichtsmatrizen legen wir anhand der Anzahl der Neuronen in der Eingabeschicht bzw. der Versteckten- und der Ausgabeschicht fest. Diese drei Parameter sind also Eingaben, die unsere Funktion benötigt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f825d638",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(input_nodes, hidden_nodes, output_nodes):\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b64eb7b",
   "metadata": {},
   "source": [
    "Jetzt müssen wir die Gewichte initialisieren. Sie sind essenziell in einem Neuronalen Netzwerk, da man durch die Verfeinerung und Anpassung der Gewichte auf das gewünschte Ergebnis kommt. Gehen wir vom folgendem Neuronalen Netzwerk aus:\n",
    ">NN mit Eingabe, versteckter und ausgabeschicht\n",
    "\n",
    "Wir benötigen eine Matrix mit den Gewichten für die Verbingungen zwischen der Eingabeschicht und der ersten versteckten Schicht. Diese kürze ich so ab: $w_{1}$\n",
    "\n",
    "Zudem benötigen wir eine weitere Matrix für die Verknüpfungen zwischen der versteckten Schicht und der Ausgabeschicht: $w_{2}$.\n",
    "\n",
    "Wie initialisieren wir die Gewichte? Die Anfangswerte der Gewichte können grundsätzlich zufällig gewählt werden, es sollte jedoch beachtet werden, dass die Werte nicht gross sind, um unnötige Komplikationen zu vermeiden. Probiere die folgenden Funktionen aus, welche eignen sich gut? Bevor man diese Funktionen verwenden kann, muss die Bibliothek numpy importiert werden, welche beliebige algebraische Operationen ausfühern kann. Mit folgendem command ist das möglich: <code>import numpy as np</code>\n",
    "\n",
    "<pre><code>1. np.random.rand(X, X)\n",
    "2. np.random.random([X, X])\n",
    "3. np.random.randn(X, X)\n",
    "4. np.zeros([X, X])\n",
    "5. np.random.randint(X, size=(X, X))</code></pre>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b40be6a",
   "metadata": {},
   "source": [
    "Wir werden die numpy-Funktion \"*np.random.rand(Zeilen, Spalten)*\" verwenden. Diese generieret zufällige Werte zwischen 0 und 1. Da wir aber auch an negativen Werten interessiert sind, subtrahieren wir einfach noch 0.5 vom Ergebnis, so erhalten wir Werte zwischen -0.5 und 0.5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e971c59a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.16209822, -0.25296615,  0.30172713],\n",
       "       [ 0.16008349,  0.08877175, -0.28466904],\n",
       "       [ 0.45992348, -0.00945197, -0.41981435]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.rand(3,3) - 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4a47a8",
   "metadata": {},
   "source": [
    "Nun können wir diese Operation analog in unser Netzwerk übertragen. Die Frage ist nur noch, wie gross sollen diese beiden Matrizen werden? Dies kann man sich gut überlegen. Angenommen das neuronale Netzwerk hat 3 Eingabeneuronen und 3 in der versteckten Schicht und alle 3 Eingabeneuronen sind mit allen 3 Neuronen der versteckten Schicht verbunden. Das heisst jedes Neuron der Eingabeschicht hat jeweils 3 Verbindungen. Insgesamt sind es $3 \\times 3$. Die Grösse der Gewichtsmatrix ist also abhängig von der Anzahl der Neuronen der verbindenden Schichten. Dies muss unbedingt berücksichtigt werden im Code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f29023",
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = np.random.rand(hidden_nodes, input_nodes) - 0.5  # Verknüpfungsgewichte zwischen Input layer und hidden layer \n",
    "W2 = np.random.rand(output_nodes, hidden_nodes) - 0.5  # Verknüpfungsgewichte zwischen hidden layer und output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8106c603",
   "metadata": {},
   "source": [
    "Bis jetzt haben wir ein Grundgerüst für unser neuronales Netzwerk erstellt und die Gewichte mit einer Funktion zufällig initialisiert. Nun können wir uns der nächsten Aufgabe widmen - forwardpropagation. \n",
    "\n",
    "## Forward Propagation\n",
    "\n",
    "Das Netzwerk soll eine erste Ausgabe generieren. Das Netz bekommt eine Eingabe, diese wird aufgrund der Gewichte modereriert. Anschliessend transformiert eine Aktivierungsfunktion die summierten Eingangssignale in das Ausgangssignal der jeweiligen Schichten. Wie bereits beim Perzeptron erklärt ist der [erster Schritt](#1) die Summe aller multiplizierten Werte. Möglich mithilfe des Punktprodukts, bzw. des Matrixprodukts:\n",
    "\n",
    "$$Z_{1} = w_{1} \\cdot X$$\n",
    "Hier Steht das $Z_1$ für die Eingaben in die erste versteckte Schicht, welche dem Matrixprodukt zwischen den Eingaben ($X$) und den Gewichten ($w_{1}$) entspricht. Dieses $Z_1$ benötigen wir nämlich im nächsten Schritt als Eingabe in die Aktivierungsfunktion. Die Matrixmultiplikation müssen wir aber nicht einzeln für jedes Neuron durchführen. Hier kommt die numpy-Bibliothek ins Spiel. Mit der Funktion *np.dot()* können wir ganz einfach das Punktprodukt zweier Matrizen berechnen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96da45f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z1 = np.dot(W1, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2ea9f7",
   "metadata": {},
   "source": [
    "Auch der die Eingaben in die zweite Schicht $Z2$ sind sehr einfach zu schreiben. Anstatt des Parameters $X$ verwenden wir $A1$. Dieser steht für die Ausgaben aus der ersten Schicht, welche die Eingaben der zweiten bilden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b923cee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z2 = np.dot(W2, A1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50df043",
   "metadata": {},
   "source": [
    "Das ist bereits der ganze Code.<p> &#128516;</p>\n",
    "Diese zwei Linien Code erledigen die gesamte Arbeit für uns. Sie müssen nicht einmal geändert werden, sollte man die Anzahl der Neuronen ändern. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7445a711",
   "metadata": {},
   "source": [
    "Nun fehlt  nur noch [Schritt 2](#2)- die Aktivierungsfunktion. Drei verschiedene werde ich kurz ansprechen.\n",
    "### Sigmoid\n",
    "Diese Funktion generiert Werte zwischen 0 und 1. 0 steht für negative Unendlichkeit. Bei der Eingabe von 0 gibt sie den Wert 0,5 aus und 1 steht für positive Unendlichkeit. Die Formel der Sigmoidfunktion sieht so aus: $$y= \\frac {1}{1+e^{-x}} $$\n",
    "\n",
    "Als Funktion in Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4d6ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sigmoid(inputs): \n",
    "    E = math.e \n",
    "    return 1/(1+E**(-inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3bcd92",
   "metadata": {},
   "source": [
    "$E$ steht hier für die Eulersche Zahl, nachdem Mathematiker Leonard Euler. Um die Zahl nicht ausschreiben zu müssen importieren wir sie mithilfe der Aussage <code>math.e</code> und nennen sie $E$.\n",
    "\n",
    "Die Sigmoid-Funktion, die in der Vergangenheit in versteckten Schichten verwendet wurde, wurde schließlich ersetzt durch die \n",
    "Rectified Linear Units Aktivierungsfunktion (oder ReLU)\n",
    "\n",
    "### Rectified Linear Unit (ReLU)\n",
    "Die ReLu Aktivierungsfunktion ist eine lineare Funktion, welche positive Werte erzeugt und alle negativen auf Null setzt.\n",
    "Die Formel der ReLu : $$y = max (0,x) $$\n",
    "\n",
    "$max()$, nimmt im Grunde einfach den höheren Wert entweder $0$ oder $x$ als Ausgabewert. Eine negativer Wert für $x$ wird nämlich auf Null gesetzt, da null zwingend grösser ist als ein negativer Wert. Jeder positive Wert von $x$ ist eine Ausgabe, da er höher als null ist.\n",
    "\n",
    "Auch in python ist diese Funktion einfach auszudrücken:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3f3ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(x):\n",
    "    return np.maximum(x, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04aeaf6",
   "metadata": {},
   "source": [
    "### Softmax\n",
    "Die Softmax-Aktivierungsfunktion wandelt kurz gesagt die Werte ihrer Eingabe in Wahrscheinlichkeiten um. Wenn ein Vektor die Eingabe bildet so wird jeder Wert des Vektors durch die Summe aller Werte geteilt.\n",
    "Die entsprechende Formel ist: $$S_{i,j} = \\frac {e^{z_{i,j}}}{\\sum_{l=1}^Le^{z_{i,l}}}$$\n",
    "\n",
    "Auch das ist nicht schwer in Code auszudrücken:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fc5b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    A = np.exp(x) / sum(np.exp(x))\n",
    "    return A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fceace",
   "metadata": {},
   "source": [
    "Wie erwähnt bildet die Summe aller Eingaben multipliziert mit den Gewichten nun die Eingabe für die Aktivierungsfunktion, d.h $Z_1$. Ich werde von nun an die Sigmoid-Funktion verwenden. Das ist ganz einfach zu notieren: <code>A1 = Sigmoid(Z1)</code>\n",
    "\n",
    "analog dazu: <code>A2 = Sigmoid(Z2)</code>\n",
    "\n",
    "Ein Detail, welches man auf keinen Fall vergessen sollte, sind die Parameter, welche unsere Funktion <code>forward_prop()</code> benötigt. Zum einen sind es die beiden Gewichtsmatrizen $W_1$ bzw. $W_2$, zum anderen die Daten $X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34ca9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(input_nodes, hidden_nodes, output_nodes):\n",
    "    \n",
    "    W1 = np.random.rand(hidden_nodes, input_nodes) - 0.5  # Erste versteckte Schicht   \n",
    "    W2 = np.random.rand(output_nodes, hidden_nodes) - 0.5  # Zweite versteckte Schicht\n",
    "\n",
    "    return W1,  W2\n",
    "\n",
    "def Sigmoid(inputs): \n",
    "    E = math.e \n",
    "    return 1/(1+E**(-inputs))\n",
    "\n",
    "\n",
    "def forward_prop(W1, W2, X):\n",
    "    \n",
    "    Z1 = np.dot(W1, X)\n",
    "    A1 = Sigmoid(Z1)\n",
    "    Z2 = np.dot(W2, A1)\n",
    "    A2 = Sigmoid(Z2)\n",
    "    \n",
    "    return Z1, A1, Z2, A2\n",
    "\n",
    "\n",
    "def backward_prop():\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa948cc2",
   "metadata": {},
   "source": [
    "Jetzt fehlt nur noch \"backward_prop()-Funktion\". Bevor wir uns jedoch mit dieser beschäftigen, sollte man nun einmal probieren ein kleines Netz zu erstellen, um Fehlerquellen frühzeitig zu bemerken, falls vorhanden. Hierzu kann man eine kleine Funktion erstellen, welche als Eingabe die Inputs $X$, die Anzahl der Eingabe-, Versteckten- und Ausgabeschicht benötigt. Nun kann man in dieser Funktion die zwei Gewichte initialisieren. Zudem muss führt man die <code>forward_prop</code> mit Z1,A1,Z2,A2 durch. \n",
    "\n",
    "Nun kann man die Anzahl der Neuronen bestimmen und das Netz mit zufällig gewählten Eingangswerten abfragen. Bsp:<code>X = [1.0, 0.5, -1.5, 0.5]</code>\n",
    "***\n",
    "<b> <summary> <font color=\"red\"><b> Eine mögliche Lösung</b></font></summary> </b> <details>\n",
    " \n",
    "<pre><code>    \n",
    "input_nodes= 4\n",
    "hidden_nodes= 10\n",
    "output_nodes= 4    \n",
    "    \n",
    "X = [1.0, 0.5, -1.5, 0.5]\n",
    "    \n",
    "def test(X, input_nodes, hidden_nodes, output_nodes):\n",
    "    W1, W2 = init_params(input_nodes, hidden_nodes, output_nodes)\n",
    "    Z1, A1, Z2, A2 = forward_prop(W1,W2, X)\n",
    "    print(Z2)\n",
    "    return W1,  W2\n",
    "    \n",
    "W1, W2 = test(X, input_nodes, hidden_nodes, output_nodes)\n",
    "</code></pre>    \t\n",
    "</details>\n",
    "\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4e40aa",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Nun kommen wir zum spannenderen Abschnitt- und zwar wie ein Neuronales Netzwerk lernt. Wie bringen wir ihm die Ziffern von 0 bis 9 bei und was tun wir wenn es eine falsche Annahme tätigt? Der nächste Abschnitt versucht diese Fragestellungen verständlich zu beantworten. \n",
    "Wichtig ist zu wissen: Das Training besteht aus zwei Teilen:\n",
    " - Der erste Durchlauf mit einem Trainingsbeispiel, der eine erste Ausgabe liefert. Das haben wir aber bereits gemacht, mit der forwardpropagation\n",
    " - Backpropagation - d.h. die berechnete Ausgabe im ersten Teil mithilfe der anpassbaren Gewichten so aktualisieren, dass sie sich der gewünschten Ausgabe annähern\n",
    " \n",
    "Der erste Teil unterscheidet sich nicht von der <code>forward_prop-Funktion</code>. Die Ausgaben diser Funktion bilden somit auch die Eingaben in die <code>backward_prop</code> Funktion. Um das Netzwerk zu trainieren benötigen wir nun nicht nur eine \"X\" sondern auch eine \"Y\" mit allen Zielwerten. Wir müssen ja irgenwie überprüfen können ob unser Netzwerk richtig liegt und mit dem \"Y\", welche das erwünschte Ergebnis enthält, ist das möglich. \n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b><p style=\"font-size:20px\">&#128161;</p></b> <p> Diese Methode des Lernverfahrens nennt man übrigens <mark>supervised learning</mark> , da man im Besitz der erwünschten Zielwerte ist und das neuronale Netzwerk so überewacht und trainiert.</p>\n",
    "</div>\n",
    "\n",
    "Zudem benötigen wir noch eine Lernrate als Eingabe, um unsere Gewchte später zu aktualisieren."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b34187f",
   "metadata": {},
   "source": [
    "\n",
    "***\n",
    "<b> <summary> <font color=\"red\"><b>  Vollständiger Code bis jetzt</b></font></summary> </b> <details>\n",
    "   \n",
    "<pre><code>import numpy as np\n",
    "import math\n",
    "\n",
    "def init_params(input_nodes, hidden_nodes, output_nodes):\n",
    "    W1 = np.random.rand(hidden_nodes, input_nodes) - 0.5 \n",
    "    W2 = np.random.rand(output_nodes, hidden_nodes) - 0.5 \n",
    "\n",
    "    return W1,  W2\n",
    "    \n",
    "def Sigmoid(inputs): \n",
    "    E = math.e \n",
    "    return 1/(1+E**(-inputs))\n",
    "    \n",
    "def forward_prop(W1, W2, X):\n",
    "    Z1 = np.dot(W1, X)\n",
    "    A1 = Sigmoid(Z1)\n",
    "    Z2 = np.dot(W2, A1)\n",
    "    A2 = Sigmoid(Z2)\n",
    "    return Z1, A1, Z2, A2\n",
    " \n",
    "def backward_prop(Z1, A1, Z2, A2, W1, W2, X, Y, alpha):\n",
    "    return W1, W2\n",
    "\n",
    "</code></pre>\n",
    "</details>\n",
    "\n",
    "***"
   ]
  },
  {
   "attachments": {
    "image-2.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "42ed7339",
   "metadata": {},
   "source": [
    "Jetzt kommen wir zum komplexen und zugleich spannensten Teil: Die Aktualisierung der Gewichte aufgrund der entstandenen Fehler. Zuerst muss aber das Prinzip der Backpropagation erläutert werden.\n",
    "\n",
    "## Backpropagation\n",
    "\n",
    " - Forwardpropagation: Daten in Vorwärtsrichtung durch das Netzwerk bewegen\n",
    " - Backpropagation : Den Ausgabefehler in Rückwärtsrichtung durch das Netzwerk bewegen und somit die Gewichte zu aktualisieren\n",
    "\n",
    "Bis jetzt haben wir die Eingabedaten einmal durch unser Netzwerk, von der Eingabeschicht, über die Versteckteschicht, bis zur Ausgabeschicht bewegt und zum Schluss erhalten wir ein Ergebnis. Nun wollen wir uns mit diesen Ausgabewerten befassen. Da wir ja im Besitz der korrekten Werte sind, wissen wir wie gut die Vorhersage unseres Netzwerks war. Das Ziel ist es nun mithilfe der Zielwerte, unser Netzwerk zu verbessern, sodass es möglichst fehlerlos funktionieren kann. Dieses Thema ist unglaublich kompliziert und erfordert viele Kenntisse in der Mathematik. Ich kann und werde nicht alles genau erklären können, vielmehr probiere ich einen grundlegenden Überblick über das Thema zu verschaffen, sodass der Begriff \"Backpropagation\" verständlicher wird. \n",
    "\n",
    "Fangen wir aber ganz einfach an. Nehmen wir an, wir haben nur ein Neuron in der versteckten Schicht welches zur Ausgabeschicht führt. Die Ausgabeschicht besitzt ebenfalls nur ein Neuron. \n",
    "\n",
    "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "<svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 154.13 56.83\"/>![image-2.png](attachment:image-2.png)\n",
    "\n",
    "Sagen wir unser Ausgabewert ist 0.5. Allgemein im supervised learing besitzt man wie erwähnt die Zielwerte bereits. Nehmen wir an, unser Zielwert ist 1. Mit diesen Informationen kann man den Fehler des Netzwerks berechnen. Auch Error genannt. Der Error ist die Differenz zwischen dem Zielwert und der Vorhersage des Netzwerkes:\n",
    "$$ Error = Zielwert - Vorhersage $$\n",
    "In unserem fall ist $Error = 0.5$. Mit dieser Information können wir uns nun überlegen, wie wir das Gewicht ändern müssen, sodass der Wert des Fehler kleiner wird. Anders gesagt, wir müssen das Gewicht erhöhen und zwar in die Richtung des Fehlers.<br> Was ist jetzt aber wenn wir zwei versteckte Neuronen haben und somit zwei Gewichte, welche unsere Ausgabe bzw. den Fehler beeinflussen. \n",
    "\n",
    ">bild von 2 neuronen verbunden mit ausgabeneuron\n",
    "\n",
    "\n",
    "\n",
    "Man muss den Fehler aufteilen auf die beiden Gewichte. Das wichtige ist aber, dass man ihn nicht gleichmässig aufteilt, sondern man teilt den Fehler proportional zu den grössen der Gewichte auf. Das bedeutet, bei den Gewichten $w_{1,1}=0.2$ und $w_{2,1}= 0.1$ gehen dementsprechend 2/3 des Ausgabefehlers zum ersten Gewicht und 1/3 zum Zweiten. <br>\n",
    "Warum macht man das so?<br> Das Beispiel soll den Einfluss der Gewichte auf die Ausgabe bzw. den Fehler darstellen.\n",
    ">[Warum spielt der Wert der Gewichte eine Rolle?](https://www.geogebra.org/graphing/fagxtb8t)\n",
    "\n",
    "Man sieht sehr schön, dass $w_{1}= 0.5$ einen grösseren Einfluss auf die Position der Linie hat. Somit beeinflusst dieses Gewicht die Ausgabe bzw. den Fehler stärker. Aus diesem Grund würde es keinen Sinn machen, den Ausgabefehler gleichmässig auf die beiden Gewichte aufzuteilen, da $w_1$ einen grösseren Einfluss auf den Fehler hat. <br>\n",
    "Die Schreibweise ist sehr verständlich und nachvollziehbar. Wie gesagt müssen wir den Fehler des neuronalen Netzwerkes aufteilen und der Anteil hängt von den grössen der Gewichte ab. Der Anteil des Fehlers um das erste Gewicht zu verfeinern lautet so:\n",
    "\n",
    "\n",
    "$$ \\frac {w_{1,1}}{w_{1,1}+w_{2,1}}$$ \n",
    "\n",
    "\n",
    "Das macht Sinn. So sind die Anteile immer proportional zu den Grössen der Gewichte. Analog dazu der Anteil für das zweite Gewicht:\n",
    "\n",
    "\n",
    "$$ \\frac {w_{2,1}}{w_{1,1}+w_{2,1}}$$\n",
    "\n",
    "***\n",
    "<b> <summary> <font color=\"red\"><b> kurzes Beispiel</b></font></summary> </b> <details>\n",
    "   \n",
    "Angenommen das erste Gewicht ist  dreimal grösser als das zewite. $w_{1,1}=3, w_{2,1}= 1$. So ergibt sich für den Anteil des ersten Gewichts: \n",
    "\n",
    "\n",
    "$$\\frac {w_{1,1}}{w_{1,1}+w_{2,1}} = \\frac {3}{3+1}= \\frac {3}{4}$$\n",
    "\n",
    "\n",
    "und für den Anteil des zweiten Gewichts:\n",
    "\n",
    "\n",
    "\n",
    "$$\\frac {w_{2,1}}{w_{1,1}+w_{2,1}} = \\frac {1}{3+1}= \\frac {1}{4}$$\t\n",
    "\n",
    "    \t\n",
    "</details>\n",
    "\n",
    "***\n",
    "\n",
    "\n",
    "Das heisst wir können eine Gleichung aufstellen, welche den Fehler der versteckten Schicht beschreibt, welche direkt mit dem Ausgabeneuron verbunden ist.\n",
    "\n",
    "$$ e_{h1}= \\frac {w_{1,1}}{w_{1,1}+w_{2,1}} \\times e_1$$\n",
    "\n",
    "und gleichermassen:\n",
    "\n",
    "$$ e_{h2}= \\frac {w_{2,1}}{w_{1,1}+w_{2,1}} \\times e_1$$\n",
    "\n",
    " - $e_{h1}$ bzw.   $e_{h2}$ stehen für die Fehler in der versteckten Schicht\n",
    " - $w_{1,1}$ bzw.$w_{2,1}$ stehen für die Verbindungsgrwichte\n",
    " - $e_1$ steht für den Ausgabefehler des Netzwerks \n",
    " \n",
    " \n",
    "Diese Idee der proportionalen Aufteilung des Fehlers kann auf beliebig viele Neuronen angewendet werden. Das heisst bei 50 Neuronen die mit einem Ausgabeneuron verbunden wären, würde der Ausgabefehler proportional auf die 50 Gewichte verteilt.<br>  \n",
    "\n",
    "Nun wollen wir uns das Szenario anschauen, im Falle von zwei Ausgabeneuronen.\n",
    ">Bild mit zwei hidden neuronen verbunden mit zwei ausgabeneuronen\n",
    "\n",
    "Das bedeutet auch, wir haben Zwei Ausgabefehler $e_1$ bzw. $e_2$. Diese müssen natürlich berücksichtigt werden in unserer Gleichung, denn nun werden die beiden Fehler $e_{h1}$ und $e_{h2}$ von beiden Ausgabefehlern beeinflusst. Die Gleichung für $e_{h1}$ verändert sich folgendermassen:\n",
    "\n",
    "$$ e_{h1}= \\frac {w_{1,1}}{w_{1,1}+w_{2,1}} \\times e_1 + \\frac {w_{1,2}}{w_{1,2}+w_{2,2}} \\times e_2$$\n",
    "\n",
    "und gleichermassen:\n",
    "\n",
    "$$ e_{h2}= \\frac {w_{2,1}}{w_{1,1}+w_{2,1}} \\times e_1 + \\frac {w_{2,2}}{w_{1,2}+w_{2,2}} \\times e_2$$\n",
    "\n",
    "\n",
    "Die wichtigste Frage mit der wir uns beschäftigen müssen, ist, wie berechnet man den Fehler eines Neurons an einem beliebigen Ort in einem Netzwerk? Denn sobald dieses nichtmehr direkt mit der Ausgabeschicht verbunden ist, kann die einfache Formel \"Error = Zielwert - Vorhersage\" nicht mehr so angewendet werden.   \n",
    "\n",
    "\n",
    "\n",
    "Funktion wenn man die Normalisierung der Gewichte streicht:\n",
    "\n",
    "$$ e_{h1}=\\color {Blue} w_{1,1} \\times e_1 + \\color {Red}w_{1,2}\\times e_2 $$\n",
    "\n",
    "\n",
    "$$e_{h2}=\\color {Blue} w_{2,1} \\times e_1 +\\color {Red} w_{2,2}\\times e_2$$\n",
    "Wenn man sich diese beiden Gleichungen anschaut dann fällt auf, man kann sie auch als Matrixmultiplikation schreiben:\n",
    "\n",
    "\n",
    "$$\\begin{bmatrix} \\color {Blue} w_{1,1}&\\color {Red}w_{1,2}\\\\\\color {Blue} w_{2,1}&\\color {Red}w_{2,2}\\end{bmatrix} \\cdot \\begin{bmatrix}  e_1\\\\ e_2 \\end{bmatrix}$$\n",
    "\n",
    "\n",
    "Diese Multiplikation liefert uns dasselbe Ergebnis. Das heisst dieses Matrixprodukt liefert uns die Fehler der versteckten Schicht. Eine weitere Besonderheit fällt auf, sobald man die Matrix der Gewichte betrachtet. Wenn man sich die Matrix der Gewichte anschaut, welche man in die Vorwärtsrichtung benötigt sieht diese folgendermassen aus:\n",
    "\n",
    "$$\\begin{bmatrix} w_{1,1}&w_{2,1}\\\\ w_{1,2}&w_{2,2}\\end{bmatrix}$$\n",
    "\n",
    "Diese sieht der oben gezeigten Matrix sehr ähnlich, tatsächlich ist die Matrix, welche wir benötigen um den Fehler in der versteckten Schicht zu berechnen, die [transponierte](#3) Matrix der Gewichte.\n",
    "\n",
    "Das bedeutet wir haben eine neue Formel für unseren Fehler:\n",
    "\n",
    "$$ error_{versteckt} = w_{ho}^{T} \\cdot error_{ausgabe}$$\n",
    "> warum kann man normalisierung weglassen?\n",
    "\n",
    ">erklären was backpropagation ist\n",
    "\n",
    "Nun wollen wir uns mit der Frage beschäftigen, wie  man die Verknüpfungsgewichte eigentlich aktualisiert? Wir haben bereits einen wichten Teil erarbeitet, welcher uns helfen soll die Gewichte zu aktualisieren- den Fehler berechnet. Um die Gewichte zu aktualisieren benötigen wir nämlich den Fehler.\n",
    "\n",
    "Um auf die gestellte Frage zurückzukommen, es gibt eine Technik names \"gradient descent\" oder Gradientverfahren auf deutsch. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f27a4d",
   "metadata": {},
   "source": [
    "## Gradientverfahren\n",
    "\n",
    "Dieses Verfahren ist sehr komplex und bringt viele Hürden mit sich. Ich probiere einen Überblick zu verschaffen, werde aber zudem noch andere Quellen angeben, welche das Verfahren verdeutlichen sollen.\n",
    "\n",
    "Der folgende Textabschnitt soll dieses Verfahren verständlicher machen.\n",
    ">Stellen Sie sich eine sehr komplizierte Landschaft vor mit Bergspitzen und Tälern sowie Bergen mit tückischen Unebenheiten und Spalten. Es ist finster, und Sie können nichts sehen. Sie wissen, dass Sie sich auf einer Anhöhe befinden und ganz nach unten gelangen müssen. Von der gesamten Landschaft besitzen Sie keine genaue Karte. Allerdings haben Sie eine Taschenlampe. Was tun Sie jetzt? Wahrscheinlich werden Sie im Schein der Taschenlampe den Boden in der nahen Umgebung inspizieren. Weiter entferntes Gelände ist überhaupt nicht zu sehen und gleich gar nicht die gesamte Landschaft. Sie können erkennen, welcher Teil des Bodens anscheinend nach unten führt, und kleine Schritte in dieser Richtung gehen. Auf diese Weise tasten Sie sich langsam den Berg hinunter, immer Schritt für Schritt, ohne eine vollständige Karte zu besitzen und ohne im Voraus eine Route geplant zu haben.\n",
    "Die mathematische Version dieses Konzepts heißt \"Gradientenverfahren\" oder auch \"Verfahren des steilsten Abstiegs\" - es dürfte klar sein, warum. Nachdem Sie einen Schritt gegangen sind, untersuchen Sie wieder die nahe Umgebung, um festzustellen, in welcher Richtung Sie Ihrem Ziel näher kommen. Dann wagen Sie erneut einen Schritt in diese Richtung. Das setzen Sie so lange fort, bis Sie glücklich am Fuß der Berge angekommen sind. Der Gradient entspricht der Bodenneigung. Sie gehen in die Richtung, wo die Neigung am steilsten nach unten führt.\n",
    ">>-Tariq Rashid\n",
    "\n",
    "Wir verwenden also das Gradientverfahren um uns an das Minimum einer Funktion anzunähern. Im Falle unseres neuronalen Netzwerkes steht der Ausgabefehler für diese Funktion. Das bedeutet mit dem Gradientverfahren nähern wir uns Schritt für Schritt dem kleinsten Wert des Fehlers an. Beim annähern können wir unsere Schritte verkleinern, bis wir an der erwünschten Position angelangt sind. In dem wir den Fehler minimieren, verbessern wir die Ausgabe des Netzwerkes.\n",
    "\n",
    "Wenn man sich dieses Verfahren anhand einer Funktion wei $y = x^2$ vorstellt, dann sieht es nicht allzu kompliziert aus. Dies ist üblicherweise jedoch nicht der Fall. Komplexe Funktionen hängen oftmals von vielen Parameter ab, vergleichbar mit unserer Fehlerfunktion, welche von den vielen Gewichten abhänging ist. Da wir das globale Minimum der Fehlerfunktion ansteuern, um so die Netzwerkausgabe zu verbessern, stellen sogenannte [lokale Minima](https://de.wikipedia.org/wiki/Extremwert#/media/Datei:Extrema_example_de.svg) eine Hürde für uns dar. Um das Szenario zu vermeiden, in einem lokalen Minimum zu landen, müssen wir unser Netzwerk bloss einige Male mit verschiedenen Gewichten trainieren. \n",
    "\n",
    ">[Visualiserirung](https://www.geogebra.org/calculator/cmez9wzm)\n",
    "\n",
    "\n",
    "Die essenzielle Funktion die wir benötigen um unsere Gewichte zu aktualisieren, ist die Fehlerfunktion des neuronalen Netzwerkes. Diese haben wir ja bereits und ist ganz einfach zu notieren: $Fehler = Zielwert - Ausgabe$. Aus verschiedensten Gründe ua. um Nullwerte zu vermeiden werden wir jedoch das Quadrat der Differenz verwenden. D.h. $Fehler = (Zielwert - Ausgabe)^2$\n",
    "\n",
    "Um uns dem globalen Minimum der Funktion zu nähern, müssen wir die Steigung der Funktion im Bezug auf die Gewichte berechnen. Die Frage ist wie der Fehler beeinflusst wird, sobald Änderungen bei den Gewichten vorgenommen werden. Dabei sind wir logischerweise auf die **Differenzialrechnung** angewiesen.\n",
    "***\n",
    "Als Hilfe sei hier nochmals auf die Videos von [3Blue1Brown](https://youtube.com/playlist?list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr&si=YQ4vBxmPSneyBYo4) verwiesen \n",
    "***\n",
    "Der oben bereits erwähnte Fall kann man folgendermassen als Ausdruck aufschreiben:\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial w_{xy}} $$\n",
    "\n",
    "Dieser Ausdruck besagt: wie verändert sich der Fehler $E$, wenn man den Wert des Gewichts $w_{xy}$ ändert.\n",
    "\n",
    "Dieser Ausdruck ist sehrwahrscheinlich nicht intuitiv verständlich, deshalb möchte ich zuerst auf ein einfacheres Szenario zurückgreifen und später wieder auf diesen Ausdruck zurückkommen.\n",
    "\n",
    "\n",
    "***\n",
    "<b> <summary> <font color=\"red\"><b> Einfacheres Beispiel </b></font></summary> </b> <details>\n",
    "\n",
    "Gehen wir von folgender Situation aus: Ansatt einer komplizierten Funktion haben wir nur die Funktion $y = mx $. Nehmen wir an $x$ sind unsere Eingaben in das Netzwerk und $y$ sind die Ausgaben. Die Zielewerte nennen wir $\\hat y$. Das ergibt:\n",
    "$$error = y - \\hat y$$\n",
    "\n",
    "Soweit so gut.<br>\n",
    "Das Ziel ist es ja unser Modell zu trainieren, sodass es möglichst genaue Ausgaben liefert. Um ein Modell zu trainieren, optimieren wir die Gewichte, um die Genauigkeit und Zuverlässigkeit des Modells zu verbessern. Dafür brauchen wir einen Weg oder eine Methode, mitwelcher wir bestimmen können wie richtig/falsch unser Netzwerk ist.\n",
    "An dieser Stelle kommt eine neue Funktion ins Spiel- die \"**Cost function**\". Ähnlich gibt es die \"**Loss function**\". Der Unterschied zwischen diesen beiden Funktionen ist lediglich, dass die Loss function eine Funktion ist, eine Vorhersage definiert ist und diesen Fehler misst. Die Cost ist allgemeiner. Sie ist eine Summe von Verlustfunktionen über Ihren Trainingssatz. Sie ist also mehr ein Durchschnittswert aller Beispielen.  \n",
    "    \n",
    "\n",
    "Deswegen möchte ich zuerst kurz auf die Loss-funktion zusprechen kommen. Da der \"Loss\" den Fehler misst, sollte er idealerweise 0 sein. Wer sich ein bisschen mit diesen Funktionen auskennt, weiss, dass es verschiedene Arten von \"Loss\"-Funktionen gibt. Wir werden die \"squared loss\"-Funktion verwenden. In unserem Beispiel sieht diese folgendermassen aus:\n",
    "\n",
    "$$ Loss = \\sum_{i=1}^n (y - \\hat y)^2$$\n",
    "\n",
    "Wie gesagt ist unser Ziel, den Loss so weit wie möglich zu minimieren. Im Grunde genommen heisst das nichts anderes, als den Eingabewert zu finden, der uns den tiefsten Punkt der Funktion liefert. Genau das, was wir in unserem Neuronalen Netzerk tun müssen. Der Weg, welcher uns dieses Minimum liefert ist eben das Gradientverfahren, mithilfe der Differenzialrechnungen. Ich habe probiert eine kleine Visualisierung zu erstellen:\n",
    "\n",
    "[Wie verhält sich die Steigung im Bezug auf unsere Ausgaben](https://www.geogebra.org/calculator/wgjtcrff)\n",
    "\n",
    "Es ist gut erkennbar, dass sich die Steigung ändert, sobald sich die Ausgaben verändern. Und zwar je näher wir uns auf das Minimum, unser Ziel, bewegen, desto kleiner wird die Steigung. Das bedeutet, anhand der Steigung können wir uns einfach dem Minimum nähern.<br>\n",
    "Was wir nun berechnen wollen ist eben die Steigung der Funktion, um somit unsere Gewichte zu aktualisieren. Die Steigung einer Funktion wird bekannterweise durch die Ableitung berechnet.\n",
    "\n",
    "Zurück zur \"Loss function\". Wir müssen sie ein wenig vereinfachen. Anstatt die Funktion allgemein für alle Werte von $i=1$ bis $n$ zu formulieren, konzentrieren wir uns auf jeden einzelnen Fehler Schritt für Schritt. Dann können wir den Ausdruck $\\sum_{i=1}^n$ weglassen. Des weiteren erstetzen wir die Differenz \"$y - \\hat y$\" einfach mit \"$Error$\". Dann sieht es schon viel einfacher aus:\n",
    "\n",
    "$$L = (Error)^2$$\n",
    "\n",
    "Wie gesagt ist es unser Ziel den Loss zu minimieren und zwar mithilfe unseres einzigen Parameters $m$. Wie verändert sich der Loss, wenn man m verändert. Hier kommt die Ableitung ins Spiel, welche genau das als Fraktion ausdrückt, was wir suchen:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial m}$$\n",
    "\n",
    "Jetzt kommen wir in den Gebrauch der  [Chain Rule](#4). Wir können diese Verwenden, da Loss eine Funktion ist, welche abhängig vom Error ist und Error ist eine Funktion, welche abhängig von m  ist. \n",
    "\n",
    "***\n",
    "\n",
    "\n",
    "$$Loss = (Error)^2$$\n",
    "\n",
    "$$Error = mx -y$$\n",
    "    \n",
    "    \n",
    "Da: $Error = y -  \\hat y = mx - \\hat y$  \n",
    "    \n",
    "Unsere Vorhersage ($y$) entsteht ja, in dem wir die Eingaben ($x$) mit dem Parameter ($m$) multiplizieren.  \t\n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Die Anwendung der Regel liefert uns die Gleichung:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial m} = \\color{Blue}{\\frac{ \\partial L }{\\partial Error}} \\cdot \\color{Red}{\\frac{\\partial Error}{\\partial m}}$$\n",
    "\n",
    "\n",
    "Die  [Power Rule](#5) klärt uns den ersten Ausdruck:\n",
    "\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial m} = \\color{Blue}{2\\times Error} \\cdot \\frac{\\partial Error}{\\partial m}$$\n",
    "\n",
    "Um den zweiten Ausdruck zu verstehen müssen wir uns nochmals die Gleichung  Des Errors anschauen: $Error = mx -y$\n",
    "Die Ableitung von Konstaten ergibt immer null, deshalb können wir $y$ ignorieren. Die Ableitung vom Ausdruck $mx$ sollte nichmehr schwer sein. die Antwort ist $x$. Eingesetzt in die Gleichung:\n",
    "\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial m} = \\color{Blue}{2\\times Error} \\cdot \\color{Red}x$$\t\n",
    "    \n",
    "    \n",
    "Der konstante Faktor 2 spielt für uns keine grosse Rolle, unteranderem aus dem Grund, dass wir diesen Ausdruck später sowieso mit einer Lernrate multiplizieren werden. Wir können die 2 also auch weglassen. Wir erhalten diesen einfachen Ausdruck:  \n",
    "    \n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial m} =  (y - \\hat y)\\cdot x$$\t\n",
    "\n",
    "\n",
    "</div>\n",
    "  \n",
    "Wie wollen wir das Gewicht ($m$) also verbessern? Die oben gezeigte Gleichung liefert uns die Antwort. Nun fügen wir noch eine Lernrate ($\\alpha $)hinzu. Diese sagt beeinflusst eigentlich nur die Grösse des Schrittes, den wir machen wollen. Was wir nun machen, um $m$ zu verbessern: wir subtrahieren von $m$ die Multiplikation der Lernrate und des erarbeiteten Ausdrucks und wir haben unsere Lösung:\n",
    "    \n",
    "$$ m^+= m - \\alpha \\times  (y - \\hat y) \\cdot x$$\n",
    "    \n",
    "Hier haben wir die einfache Formel $y=mx$ benutzt. Ich hoffe dieses Beispiel der Lossfunktion hat das Vorgehen gut aufgezeigt. Zuerst wenden wir die Kettenregel an und bekommen somit mehrere Fraktionen. Im zweiten Schritt berechnen wir die Ableitung dieser Fraktionen. \n",
    "   \n",
    "In unserem Neuronalen Netzwerk haben wir grundsätzlich den selben Aufbau, nur mit mehreren Dimensionen. Diese Logik probieren wir nun auf unser Netzwerk zu übertragen. \n",
    "</details>\n",
    "\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff16ad8",
   "metadata": {},
   "source": [
    "Wir probieren uns nun an dem erhaltenen Ergebnis der einfachen Funktion $y= mx$ zu orientieren, um die Gewichte unseres Neuronalen Netzwerkes zu verbessern. Um diese nochmals hervorzurufen:\n",
    "\n",
    "$$ m^+ = m- \\alpha \\times Error \\times Eingaben$$\n",
    "\n",
    "Wenn wir uns nur einmal die Verknüpfungsgewichte zwischen der Versteckten- und Ausgabeschicht anschauen. Wir nennen sie $w_2$. Wir können die Formel für die Aktualisierung der Gewichte leider nicht eins zu eins übernehmen. \n",
    "\n",
    "$$ w_2^+ =w_2 - \\alpha \\times Error \\times Eingaben$$\n",
    "\n",
    "Dieser Ausdruck ist fast richtig, jedoch fehlt noch ein kleiner Teil.\n",
    "Um diesen herauszufinden, rufen wir uns nochmals die Funktion unseres Netzwerks in Erinnerung:\n",
    "\n",
    "$y= Sigmoid(w \\cdot I)$ oder auch $y= \\sigma(w \\cdot I)$ geschrieben.$\\sigma$ steht hier für die Sigmoidfunktion.\n",
    "\n",
    "Zudem betrachten wir nochmals den Ausdruck $\\frac{\\partial E}{\\partial w_{i,j}}$. Unser Ziel ist es herauszufinden wie der Fehler $E$ von den Gewichten beeinflusst wird. Ich möchte das aber mithilfe der **Cost function** durchführen:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "$$Cost= \\frac{1}{m} \\sum_{i=1}^m (y_i - \\hat y_i)^2$$\n",
    "\n",
    "\n",
    "$m$ = Anzahl der Ausgabeneuronen<br>\n",
    "$y$ = Ausgabewert des n-ten Neurons<br>\n",
    "$\\hat y$ = Zielwert des n-ten Neurons<br>\n",
    "\n",
    "Was passiert hier genau? Fangen wir ganz rechts an. Die Funktion berechnet das Quadrat der Differenz des Ausgabewertes eines Neurons und des Zielwertes. Das macht sie für alle Neuronen und summiert diese Werte. Zuletzt wird diese Summe durch die Anzahl der Ausgabeneuronen geteilt, da wir einen Durchschnittswert wollen.<br>\n",
    "\n",
    "Der Wert der Cost-function gibt uns an, wie gut unser Netzwerk insgesamt ist. Je höher der Wert, desto schlechter ist unser Netzwerk. \n",
    "\n",
    "Wir gehen nun rückwärts durch unser neuronales Netzwerk und aktualisieren unsere Gewichte so, dass der Wert der Cost-function so weit wie möglich verringert wird. Wir wollen also wissen wie jedes einzelne Gewicht diesen Wert beeinflusst und wie wir unsere Gewichte dementsprechend anpassen müssen. Das machen wir mithilfe der **Ableitung**.\n",
    "\n",
    "$$\\frac {\\partial C}{\\partial w_x} $$\n",
    "\n",
    "Um diesen Wert zu berechnen müssen wir ihn zuerst ein wenig auseinander nehemen. Wenn man sich die Formel der Funktion anschaut, merkt man schnell, das der Wert der Cost-function ($C$) nicht direkt abhängig von $w$ ist. $C$ ist vielmehr abhängig von der Ausgabe ($out$) eines Neurons. \n",
    "\n",
    "$$\\frac {\\partial C}{\\partial out}$$\n",
    "\n",
    "Wichtig ist zu erkennen, dass unsere Ausgabe ($out$) nichts anderes ist, als der Wert der Aktivierungsfunktion, welche als Eingaben die gewichteten Summen benötigt. Diese Ausgabe ist also abhängig von der gewichteten Summe ($sum$) aller Eingaben. \n",
    "\n",
    "$$\\frac {\\partial out}{\\partial sum}$$\n",
    "\n",
    "Diese Summe wiederum ist abhängig von den Gewichten $w$.\n",
    "\n",
    "$$\\frac {\\partial sum}{\\partial w_x}$$\n",
    "\n",
    "Das heisst wie können die Chain-rule verwenden, welche uns dieses Ergebnis liefert:\n",
    "\n",
    "$$\\frac {\\partial C}{\\partial w_x} = \\frac {\\partial C}{\\partial out} \\cdot \\frac {\\partial out}{\\partial sum} \\cdot \\frac {\\partial sum}{\\partial w_x}$$\n",
    "\n",
    "Ein Beispiel, sodass es hoffentlich klarer wird. Schauen wir uns folgende Abbildung an:\n",
    "\n",
    "$$I_1 ---w_1---(sum_{h} || out_{h}) ---w_2---(sum_{ot} || out_{o})$$\n",
    "\n",
    "Wir wollen nun wissen, wie wir das erste Gewicht $w_1$ anpassen sollen. Das heisst wir müssen folgenden Ausdruck berechnen:$\\frac {\\partial C}{\\partial w_1}$. Diesen Ausdruck können wir einfach anhand der oben beschriebenen Gleichung in drei Teile auseinandernehmen und Schritt für Schritt durchgehen.\n",
    "\n",
    "$$\\frac {\\partial C}{\\partial w_1} = \\frac {\\partial C}{\\partial out_{h}} \\cdot \\frac {\\partial out_{h}}{\\partial sum_{h}} \\cdot \\frac {\\partial sum_{h}}{\\partial w_1}$$\n",
    "\n",
    "<br>\n",
    "\n",
    "<a id=\"7\"></a> \n",
    "***\n",
    "<b> <summary> <font color=\"red\"><b> Schritt für Schritt Erklärung der 3 Fraktionen</b></font></summary> </b> <details>\n",
    "   \n",
    "## 1. Erste Fraktion\n",
    "<br>\n",
    "$$\\frac {\\partial C}{\\partial out_{h}}$$\n",
    "<br>\n",
    "Dieser Ausdruck ist der komplizierteste der drei, denn wir müssen ihn nochmals mithilfe der Chain-rule auseinandernehmen. Wenn man nämlich die Abbildung anschaut, fällt auf, dass die Ausgabe der versteckten Schicht ($out_h$) und der Wert der Cost-function nicht direkt voneinander abhängen. $C$ ist aber abhängig von den Ausgaben der letzten Schicht ($out_o$). Diese von der gewichteten Summe ($sum_o$) und diese wiederum von den Ausgaben der versteckten Schicht, d.h. von $out_h$. Der folgende Ausdruck ergibt sich dadurch:\n",
    "\n",
    "$$\\frac {\\partial C}{\\partial out_{h}} = \\color{Red}{ \\frac {\\partial C}{\\partial out_{o}}} \\cdot \\color{Blue}{ \\frac {\\partial out_{o}}{\\partial sum_{o}}} \\cdot \\color{Green}{ \\frac {\\partial sum_{o}}{\\partial out_{h}}}$$\n",
    "\n",
    "nun müssen wir bloss die Ableitungen dieser drei Fraktionen berechnen, was nicht sehr schwierig ist.\n",
    "\n",
    "1. $\\color{Red}{C = \\frac {1}{m} (out_{o}-\\hat y)^2}$ ergibt einfach $\\frac {1}{m} \\cdot 2(out_{o}-\\hat y)$ die zwei können wir streichen, da wir später sowieso eine Lernrate verwenden werden und die Konstante nicht wirklich einen Einfluss auf unser Ergebnis hat.<br>Somit erhalten wir: $$\\color{Red}{\\frac {1}{m} \\cdot (out_{o}-\\hat y)}$$\n",
    "<br>\n",
    "2. Als nächsten müssen wir die Ableitung dieser Formel berechnen:$\\color{Blue}{ out_o =\\frac {1}{1 + e^{-sum_{o}}}}$. Das heisst die Ableitung der Sigmoid Funktion.<br> Diese lautet so:<br>$$\\sigma'(x) = \\sigma(x) \\cdot (1-\\sigma(x))$$<br>Mit dieser Ableitung im Kopf, können wir weitermachen. Man soll sich an dieser Stelle zurückerinnern, für was die Aktivierungsfunktion zustädig ist. Einfach gesagt liefert sie uns einfach die Ausgaben einer Schicht. In unserem Fall ist das $out_o$. Setzten wir das in die Ableitung der Sigmoid-Funktion ein, erhalten wir:\n",
    "\n",
    "$$\\color{Blue}{out_o(1-out_o)}$$\n",
    "\n",
    "3. der letzte Ausdruck ist der einfachste. $\\color{Green}{sum_o = w_2 \\cdot out_{h}}$. Die Ableitung dieser Gleichung ist sehr einfach. Da wir die Abhängigkeit der Ausgabe der versteckten Schicht $out_h$ im Bezug auf $C$ wollen, werden alle anderen Parameter zu Konstanten. \n",
    "***\n",
    "<b>  <font color=\"red\"><b> kurzes Beispiel</b></font></summary> </b> <details>\n",
    "   \n",
    "Angenommen man hat drei Gewichte und drei Ausgaben der vorangehenden Schicht, weöche nun als Eingaben in die neue Schicht dienen. Das heisst, wir wenden das Punktprodukt an:\n",
    "    \n",
    "$$o_1 \\times w_1 + o_2 \\times w_2 + o_3 \\times w_3 $$\n",
    "\n",
    "Wir Erinnern uns: $f(x)=2x;f'(x)= 2$, die Konstante bleibt jeweils bei jedem Durchgang. In unserem Falle $w_1$ bzw. $w_2$ und $w_3$. Das ergibt eine Matrix mit einer Spalte:\n",
    "    \n",
    "$$\\begin{bmatrix} w_1 \\\\ w_2 \\\\w_3 \\end{bmatrix}$$\n",
    "\n",
    "\n",
    "</details>\n",
    "\n",
    "***\n",
    "Was zurückbleibt ist also eine Gewichtsmatrix. In unserem Fall ist es $w_2$.<br>Die Ableitung lautet: $$\\color{Green}{w_2}$$ \n",
    "\n",
    "    \n",
    "    \n",
    "***\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "$$\\frac {\\partial C}{\\partial out_{h}} = \\frac {1}{m} \\cdot (out_{o}-\\hat y) \\cdot out_o(1-out_o) \\cdot w_2$$\n",
    "</div>\n",
    "\n",
    "***\n",
    "\n",
    "   \n",
    "    \n",
    "## 2. Zweite Fraktion\n",
    "<br>\n",
    "$$\\frac {\\partial out_{h}}{\\partial sum_{h}}$$\n",
    "<br>\n",
    "\n",
    "Diese Ableitung wird analog zur bereits besprochenen Fraktion im ersten Teil durchgeführt.<br>\n",
    "Das ergibt:\n",
    "\n",
    "\n",
    "***\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "$$  out_h(1-out_h)$$\n",
    "</div>\n",
    "\n",
    "***\n",
    "\n",
    "## 3. Dritte Fraktion\n",
    "<br>\n",
    "$$\\frac{{\\partial sum_{h}}}{{\\partial w_1}}$$\n",
    "<br>\n",
    "\n",
    "Auch diese Ableitung ist kein Problem. Um es nochmals aufzuzeigen.<br>\n",
    "$sum_h = I_1 \\cdot w_1$. Da wir die Abhängigkeit vom Gewicht suchen wird $I_1$ zur Konstanten. \n",
    "Die Ableitung der Gleichung ergibt also:\n",
    "\n",
    "\n",
    "***\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "$$  I_1$$\n",
    "</div>\n",
    "\n",
    "***\n",
    "\n",
    "\n",
    "</details>\n",
    "\n",
    "***\n",
    "\n",
    "## Gewichtsaktualisierung\n",
    "\n",
    "Nun müssen wir die drei Einzelteile nurnoch zusammenfügen. \n",
    "\n",
    "$$ \\frac {\\partial C}{\\partial w_1} = \\frac {1}{m} \\cdot (out_{o}-\\hat y) \\cdot out_o(1-out_o) \\cdot w_2 \\times  out_h(1-out_h) \\times I_1$$\n",
    "\n",
    "Es fehlt nur noch eine Kleinigkeit. Wir wollen unsere aktualisierten Gewichte in From einer neuen Matrix haben. Wenn wir uns die Gleichung anschauen, so ergibt die Multiplikation \"$(out_o - \\hat y) \\cdot out_o(1- out_o) \\cdot w_2$\"  bloss eine Matrix mit einer Spalte, da die Matrizen elementweise miteinander multipliziert werden. Bei der zweiten Multiplikation \"$out_h(1 - out_h) \\cdot I_1$\" gibt es dasselbe Probem. Der Grund dafür ist, dass jeweils die letzte Matrix $w_2$ bzw. $I$ eine Matrix mit bloss einer Spalte ist. Die Lösung: Wir transponieren sie und erhalten somit die gewüschte Matrix.\n",
    "\n",
    "Wir erhalten die Formel, welche uns angibt, wie der Gesamte Fehler des Netzwerkes von dem einen Gewicht $w_1$ abhängt :\n",
    "\n",
    "***\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "$$ \\frac {\\partial C}{\\partial w_1} = \\frac {1}{m} \\cdot (out_{o}-\\hat y) \\cdot out_o(1-out_o) \\cdot w_2^T \\times  out_h(1-out_h) \\times I_1^T$$\n",
    "</div>\n",
    "\n",
    "***\n",
    "Den schwierigen Teil, und zwar die Cost-function abhängig vom Gewicht $w_1$ zu berechnen, haben wir geschafft.\n",
    "Mit dieser Formel können wir nun unser Gewicht $w_1$ aktualisieren. Die Gleichung, welche ich benutzen werde lautet:\n",
    "\n",
    "$$W_1^+ = W_1 - lr \\cdot \\frac {\\partial C}{\\partial w_1}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b77cf2e",
   "metadata": {},
   "source": [
    "Diesen Ausdruck in code umzusetzten ist nicht sehr schwierig. Für den Ausdruck $\\frac {\\partial C}{\\partial w_1}$ werde ich die Formulierung <code>dW1</code>bzw.<code>dW2</code> verwenden. Wir nennen die Gewichte zwischen der Eingabeschicht und der versteckte Schicht $w1$. Die Lernrate nennen wir $alpha$. Der Code für die Gewichtsaktualisierung sieht also folgendermassen aus:\n",
    "\n",
    "```\n",
    "    W1 = W1 - alpha * dW1\n",
    "    W2 = W2 - alpha * dW2\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733310f2",
   "metadata": {},
   "source": [
    "Wir brauchen jedoch noch die Formel für <code>dW2</code>\n",
    "Den Ausdruck, welchen wir für das Gewicht $w_2$ lösen müssen, heisst folgendermassen:\n",
    "\n",
    "$$\\frac {\\partial C}{\\partial w_{2}}$$\n",
    "\n",
    "Diesen können wir jedoch wieder schön auseinandernehmen:\n",
    "\n",
    "$$\\frac {\\partial C}{\\partial out_{o}} \\cdot  \\frac {\\partial out_{o}}{\\partial sum_{o}}\\cdot  \\frac {\\partial sum_{o}}{\\partial w_{2}}$$\n",
    "\n",
    "Wir haben es grundsätzlich bereits gelöst im ersten Teil der [Schritt für Schritt Erklärung.](#7). Die ersten beiden Fraktionen können wir übernehmen:\n",
    "\n",
    "$$\\frac {\\partial C}{\\partial out_{h}} = \\frac {1}{m} \\cdot (out_{o}-\\hat y) \\cdot out_o(1-out_o) \\cdot \\frac {\\partial sum_{o}}{\\partial w_{2}}$$\n",
    "\n",
    "Die letzte Fraktion ist auch sehr einfach zu lösen. Wir nehmen wieder die Ableitung von:\n",
    "\n",
    "$$w_1 \\times out_{h1} + ... +w_n \\times out_{hn} $$\n",
    "\n",
    "Nur in diesem Falle werden unsere Gewichte zu Konstanten, was uns mit $out_h$ zurücklässt. Diese Matrix müssen wir ebenfalls transponieren und wenn wir alles zusammenfügen, ergibt das:\n",
    "\n",
    "***\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "$$ \\frac {\\partial C}{\\partial w_2} = \\frac {1}{m} \\cdot (out_{o}-\\hat y) \\cdot out_o(1-out_o) \\cdot out_h^T$$\n",
    "</div>\n",
    "\n",
    "***\n",
    "\n",
    "Die Gewichtsaktualisierung des zweiten Gewichts lautet also:\n",
    "\n",
    "$$W_2^+ = W_2 - lr \\cdot \\frac {\\partial C}{\\partial w_2}$$\n",
    "\n",
    "Es fällt auf, die Gleichung der einfachen Funktion $y= mx$ ist sehr ähnlich zu unsrer: $m^+ = m- \\alpha \\times Error \\times Eingaben$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e806c0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(input_nodes, hidden_nodes, output_nodes):\n",
    "    W1 = np.random.rand(hidden_nodes, input_nodes) - 0.5  # Verknüpfungsgewichte zwischen Input layer und hidden layer \n",
    "    W2 = np.random.rand(output_nodes, hidden_nodes) - 0.5  # Verknüpfungsgewichte zwischen hidden layer und output layer\n",
    "\n",
    "    return W1,  W2\n",
    "\n",
    "def Sigmoid(inputs): #Sigmoid Aktivierungsfunktion\n",
    "    E = math.e \n",
    "    return 1/(1+E**(-inputs)) \n",
    "\n",
    "def forward_prop(W1, W2, X): #forward propagation\n",
    "    Z1 = np.dot(W1, X)\n",
    "    A1 = Sigmoid(Z1)\n",
    "    Z2 = np.dot(W2, A1)\n",
    "    A2 = Sigmoid(Z2)\n",
    "    return Z1, A1, Z2, A2\n",
    "\n",
    "def one_hot(Y):\n",
    "    one_hot_Y = np.zeros((Y.size, Y.max() + 1))\n",
    "    one_hot_Y[np.arange(Y.size), Y] = 1\n",
    "    one_hot_Y = one_hot_Y.T\n",
    "    return one_hot_Y\n",
    "\n",
    "def backward_prop(Z1, A1, Z2, A2, W1, W2, X, Y, alpha): #backwardpropagation\n",
    "    one_hot_Y = one_hot(Y)\n",
    "    dZ2 = A2 - one_hot_Y\n",
    "    dW2 = (1 / m) * np.dot(dZ2, A1.T)\n",
    "    \n",
    "    dZ1 = W2.T.dot(dZ2) * A1 * (1 - A1)\n",
    "    dW1 = (1 / m) * np.dot(dZ1, X.T)\n",
    "    \n",
    "    W1 = W1 - alpha * dW1\n",
    "    W2 = W2 - alpha * dW2\n",
    "\n",
    "    return W1, W2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8e714b",
   "metadata": {},
   "source": [
    "<code>dZ1</code> und <code>dZ2</code> habe ich hier bloss verwendet, sodass der Code nicht zu lang wird. Wie man zudem sehen kann, habe ich in diesem Code bloss einmal die Ableitung der Sigmoid-Funktion eingebaut. Die zweiten Gewichte sind garnicht davon betroffen. Das liegt daran, dass ich eine weitere Funktion verwendet habe: <code>one_hot()</code>\n",
    "\n",
    "Die Funktion initialisiert eine mit Nullen gefüllte Matrix, wobei die Anzahl der Zeilen durch die Größe der Eingaben bestimmt wird und die Anzahl der Spalten auf dem maximalen Wert der Eingaben plus eins basiert.\n",
    "\n",
    "Die Anzahl der Spalten in einer One-Hot-kodierten Matrix ist Y + 1 statt nur Y, weil die One-Hot-Kodierung eine zusätzliche Spalte enthält, um das Fehlen bekannter Kategorien darzustellen. Diese zusätzliche Spalte stellt sicher, dass die Kodierung alle möglichen Kategorien verarbeiten kann, auch wenn ein Datenpunkt zu keiner Kategorie gehört.\n",
    "\n",
    "Betrachten wir das folgende Beispiel mit drei Kategorien: A, B und C. Wir könnten diese Kategorien als 0, 1 bzw. 2 darstellen. Wenn wir eine One-Hot-Codierung ohne die zusätzliche Spalte (Y) erstellen, hätten wir nur Spalten für A und B, die wie folgt aussehen würden:\n",
    "\n",
    "Kategorie A: [1, 0]\n",
    "Kategorie B: [0, 1]\n",
    "Kategorie C: ?\n",
    "Das Problem ist, dass wir bei einem Datenpunkt der Kategorie C keine Möglichkeit haben, diesen in der One-Hot-Kodierung darzustellen, da es keine entsprechende Spalte gibt. Durch Hinzufügen einer zusätzlichen Spalte können Sie die Kategorie \"Sonstige\" oder \"Abwesenheit\" darstellen, und die Kodierung wird zu:\n",
    "\n",
    "Kategorie A: [1, 0, 0]\n",
    "Kategorie B: [0, 1, 0]\n",
    "Kategorie C: [0, 0, 1]\n",
    "Jetzt können Sie alle Kategorien darstellen, und wenn eine Kategorie abwesend ist, wird die \"andere\" Spalte (die zusätzliche) auf 1 gesetzt.\n",
    "\n",
    "Nun möchte ich erklären, was dieser Befehl bewirkt: <code>one_hot_Y[np.arange(Y.size), Y] = 1</code>\n",
    "\n",
    "Nehmen wir folgende Eingaben an: <code>Y = np.array([0, 2, 1, 2, 0])</code>\n",
    "\n",
    "Nun wollen wir unsere erstellte Matrix gefüllt mit Nullen aktualisieren, und zwar aufgrund unserer Eingaben. Das heisst wir wollen an der gegebenen Position aus den Eingaben, eine 1 in unserer Matrix erhalten. Die restlichen Positionen sollen bei null bleiben. \n",
    "\n",
    "<code>np.arange(Y.size)</code> erstellt ein Array mit ganzen Zahlen von 0 bis Y.size - 1, was den Zeilen von one_hot_Y entspricht. Das sieht wie folgt aus:\n",
    "\n",
    "[0, 1, 2, 3, 4]\n",
    "\n",
    "Y steht wie gesagt für unsere Eingaben mit den Kategoriewerten:\n",
    "\n",
    "[0, 2, 1, 2, 0]\n",
    "\n",
    "Das Ergebnis ist eine One-Hot-Codierungsmatrix, bei der jede Zeile eine andere Kategorie darstellt und jede Spalte einem bestimmten Datenpunkt entspricht, wobei eine 1 das Vorhandensein dieser Kategorie für diesen Datenpunkt anzeigt.\n",
    "\n",
    "\n",
    "In unserem Beispiel heisst das, wir verwenden die Zeilenindizes und die Werte von Y, um den entsprechenden Positionen in one_hot_Y eine 1 zuzuweisen.\n",
    "\n",
    "Hier ist die schrittweise Ausführung für die gegebene Eingabe:\n",
    "\n",
    "- Wenn Y[0] = 0 ist, wird one_hot_Y[0, 0] = 1 gesetzt.\n",
    "- Wenn Y[1] = 2 ist, wird one_hot_Y[1, 2] = 1 gesetzt.\n",
    "- Wenn Y[2] = 1 ist, wird one_hot_Y[2, 1] = 1 gesetzt.\n",
    "- Wenn Y[3] = 2 ist, wird one_hot_Y[3, 2] = 1 gesetzt.\n",
    "- Wenn Y[4] = 0 ist, wird one_hot_Y[4, 0] = 1 gesetzt.<br>\n",
    "Die endgültige one_hot_Y-Matrix sieht wie folgt aus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee840ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "array([[1., 0., 0.],\n",
    "       [0., 0., 1.],\n",
    "       [0., 1., 0.],\n",
    "       [0., 0., 1.],\n",
    "       [1., 0., 0.]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea082947",
   "metadata": {},
   "source": [
    "Nun haben wir den gesamten Code erstellt, sodass unser Netzwerk bereit ist trainiert zu werden. Dafür erstellen wir wie bereits weiteroben eine \"test funktion\" mit allen notwendigen Parametern. Das ergebnis könnte so aussehen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d64f185",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(X, Y, alpha, iterations, input_nodes, hidden_nodes, output_nodes):\n",
    "    W1, W2 = init_params(input_nodes, hidden_nodes, output_nodes)\n",
    "    for i in range(iterations):\n",
    "        Z1, A1, Z2, A2 = forward_prop(W1,W2, X)\n",
    "        W1, W2 = backward_prop(Z1, A1, Z2, A2, W1, W2, X, Y, alpha)\n",
    "   \n",
    "    return W1,  W2 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c95e089",
   "metadata": {},
   "source": [
    "Wir beginnen mit dem Initialisieren unserer Gewichte. Danach erstellen wir einen Loop, für unsere Funktion forward_prop bzw. backward_prop. Die Länge können wir anhand des Parameters **Iterations** bestimmen. \n",
    "\n",
    "Zudem könnten wir noch unsere Genauigkeit des Netzes visualisieren. ........."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb63fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions(A2):\n",
    "    return np.argmax(A2, 0)\n",
    "\n",
    "def accuracy(predictions, Y):\n",
    "    print(np.sum(predictions == Y) / Y.size)\n",
    "    \n",
    "\n",
    "def test(X, Y, alpha, iterations, input_nodes, hidden_nodes, output_nodes):\n",
    "    W1, W2 = init_params(input_nodes, hidden_nodes, output_nodes)\n",
    "    for i in range(iterations):\n",
    "        Z1, A1, Z2, A2 = forward_prop(W1,W2, X)\n",
    "        W1, W2 = backward_prop(Z1, A1, Z2, A2, W1, W2, X, Y, alpha)\n",
    "       \n",
    "        if i % 50 == 0:\n",
    "            print(\"Iteration: \", i)\n",
    "            predictions = predictions(A2)\n",
    "            print(accuracy(predictions, Y))\n",
    "    return W1,  W2 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc534e6",
   "metadata": {},
   "source": [
    "Das ist alles!!! Nun können wir unserem Netzwerk Ziffern von 0 bis 9 geben, welche es beginnt zu klassifizieren. Der letzte Schritt betrifft nur noch die Daten."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8ef24b",
   "metadata": {},
   "source": [
    "Man verwenndet grundsätzlich 3 verschiedene Arten von Daten:\n",
    "1. Trainings Daten: diese verwendet man un sein neuronales Netzwerk zu trainieren, sodass es möglichst akkurate Ausgaben erzielt.\n",
    "2. Test Daten: Die Test Daten bestehen aus einem kleinen Teil der Trainings Daten, welchen man zu beginn trennt. Dieser kleinere Datensatz wird am Schluss benutzt um zu testen, wie gut das neuronale Netzwerk wirklich performt. Es hat diesen Datensatz ja noch nie zuvor gesehen, im Gegensatz zu den Trainings Daten.\n",
    "3. Die unbekannten Datensätze: Die Daten auf welche man das Netzwerk letzen Endes anwenden möchte\n",
    "\n",
    "Die folgende Zelle zeigt einen möglichen Weg unsere Daten zu importieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26bb8fd0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(data)\n\u001b[0;32m      4\u001b[0m m, n \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(r\"train.csv\")\n",
    "\n",
    "data = np.array(data)\n",
    "m, n = data.shape\n",
    "np.random.shuffle(data)\n",
    "\n",
    "data_test = np.transpose(data[0:1000])\n",
    "Y_test = data_test[0]\n",
    "X_test = data_test[1:n]\n",
    "X_test = X_test / 255.\n",
    "\n",
    "data_train = np.transpose(data[1000:m])\n",
    "Y_train = data_train[0]\n",
    "X_train = data_train[1:n]\n",
    "X_train = X_train / 255.\n",
    "_,m_train = X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638b76e9",
   "metadata": {},
   "source": [
    "Diesen Command führen wir aus um unsere Daten zu Importieren. Ich verwende hier den MNIST-Datensatz mit handgeschriebenen Ziffern. Der Datensatz ist in diesem Fall unter dem Namen \"train.csv\" gespeichert. Das \"r\" steht für \"read\" und hat zur Folge, dass unsere Daten eingelesen werden. Dem geben wir den Namen <code>data</code>. Als nächstes transformieren wir unsere Daten in einen NumpyArray und nennen die Dimensionen <code>m</code> bzw <code>n</code>. Danach werden die Daten durchmischt und unterteilt. Die ersten 1000 Elemente sind unsere Testdaten und alle ab dem 1000sten Element sind unsere Trainingsdaten.  Nun haben wir alles geschafft und mit einem einfachen Code können wir unser Netzwerk trainieren. Beispielsweise:\n",
    "\n",
    "<code>W1, W2, = test(X_train, Y_train, 0.3, 1000, 784, 100, 10)</code>\n",
    "\n",
    "Der gesamte Code bis hierhin sollte folgendermassen aussehen. Zu oberst habe ich noch die notwendigen Bibliotheken importiert, welche wir benötigen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c179023",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "data = pd.read_csv(r\"train.csv\")\n",
    "\n",
    "data = np.array(data)\n",
    "m, n = data.shape\n",
    "np.random.shuffle(data)\n",
    "\n",
    "data_dev = np.transpose(data[0:1000])\n",
    "Y_dev = data_dev[0]\n",
    "X_dev = data_dev[1:n]\n",
    "X_dev = X_dev / 255.\n",
    "\n",
    "data_train = np.transpose(data[1000:m])\n",
    "Y_train = data_train[0]\n",
    "X_train = data_train[1:n]\n",
    "X_train = X_train / 255.\n",
    "_,m_train = X_train.shape\n",
    "\n",
    "X_train[:, 0].shape\n",
    "\n",
    "def init_params(input_nodes, hidden_nodes, output_nodes):\n",
    "    W1 = np.random.rand(hidden_nodes, input_nodes) - 0.5  \n",
    "    \n",
    "    W2 = np.random.rand(output_nodes, hidden_nodes) - 0.5 \n",
    "\n",
    "    return W1,  W2\n",
    "\n",
    "\n",
    "def Sigmoid(inputs): \n",
    "    E = math.e \n",
    "    return 1/(1+E**(-inputs))\n",
    "\n",
    "def ReLU(x):\n",
    "    return np.maximum(x, 0)\n",
    "\n",
    "def softmax(x):\n",
    "    A = np.exp(x) / sum(np.exp(x))\n",
    "    return A\n",
    "    \n",
    "def forward_prop(W1, W2, X):\n",
    "    Z1 = np.dot(W1, X)\n",
    "    A1 = Sigmoid(Z1)\n",
    "    Z2 = np.dot(W2, A1)\n",
    "    A2 = Sigmoid(Z2)\n",
    "    return Z1, A1, Z2, A2\n",
    "\n",
    "\n",
    "def Sig_deriv(x):\n",
    "    return x*(1-x)\n",
    "\n",
    "\n",
    "def ReLU_deriv(x):\n",
    "    return x > 0\n",
    "\n",
    "def one_hot(Y):\n",
    "    one_hot_Y = np.zeros((Y.size, Y.max() + 1))\n",
    "    one_hot_Y[np.arange(Y.size), Y] = 1\n",
    "    one_hot_Y = one_hot_Y.T\n",
    "    return one_hot_Y\n",
    "\n",
    "def backward_prop(Z1, A1, Z2, A2, W1, W2, X, Y, alpha):\n",
    "    one_hot_Y = one_hot(Y)\n",
    "    dZ2 = A2 - one_hot_Y\n",
    "    dW2 = (1 / m) * np.dot(dZ2, A1.T)\n",
    "    \n",
    "    dZ1 = W2.T.dot(dZ2) * A1 * (1 - A1)\n",
    "    dW1 = (1 / m) * np.dot(dZ1, X.T)\n",
    "    \n",
    "    W1 = W1 - alpha * dW1\n",
    "    W2 = W2 - alpha * dW2\n",
    "\n",
    "    return W1, W2\n",
    "\n",
    "\n",
    "def predictions(A2):\n",
    "    return np.argmax(A2, 0)\n",
    "\n",
    "def accuracy(predictions, Y):\n",
    "    print(np.sum(predictions == Y) / Y.size)\n",
    "    \n",
    "\n",
    "\n",
    "def test(X, Y, alpha, iterations, input_nodes, hidden_nodes, output_nodes):\n",
    "    W1, W2 = init_params(input_nodes, hidden_nodes, output_nodes)\n",
    "    for i in range(iterations):\n",
    "        Z1, A1, Z2, A2 = forward_prop(W1,W2, X)\n",
    "        W1, W2 = backward_prop(Z1, A1, Z2, A2, W1, W2, X, Y, alpha)\n",
    "       \n",
    "        if i % 50 == 0:\n",
    "            print(\"Iteration: \", i)\n",
    "            predictions = predictions(A2)\n",
    "            print(accuracy(predictions, Y))\n",
    "    return W1,  W2 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
